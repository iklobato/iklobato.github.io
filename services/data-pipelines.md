---
layout: default
title: Data Pipeline Engineering Services | ETL ELT Airflow Snowflake | Data Engineer
description: Build reliable data pipelines with 99.9% accuracy. Expert in Apache Airflow, Snowflake, PySpark, and Hadoop. Process millions of records with automated ETL/ELT pipelines.
keywords: Data pipeline engineering, ETL pipelines, Apache Airflow, Snowflake data warehouse, PySpark developer, Hadoop expert, data engineering services, ELT pipelines
permalink: /services/data-pipelines/
---

<header class="header">
    <div class="profile-section">
        <div class="profile-info">
            <h1>Data Pipeline Engineering</h1>
            <span class="title">Build Reliable Data Pipelines That Process Millions of Records with 99.9% Accuracy</span>
        </div>
    </div>
    
    <nav aria-label="Breadcrumb" class="breadcrumb">
        <ol itemscope itemtype="https://schema.org/BreadcrumbList">
            <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                <a href="/" itemprop="item"><span itemprop="name">Home</span></a>
                <meta itemprop="position" content="1" />
            </li>
            <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                <a href="/#services" itemprop="item"><span itemprop="name">Services</span></a>
                <meta itemprop="position" content="2" />
            </li>
            <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                <span itemprop="name">Data Pipeline Engineering</span>
                <meta itemprop="position" content="3" />
            </li>
        </ol>
    </nav>
</header>

<main class="main-content">
    <section id="service-hero">
        <div class="service-hero">
            <div class="service-hero-content">
                <h2>Build Scalable, Reliable Data Pipelines</h2>
                <p>I design and implement robust ETL/ELT pipelines orchestrated with Apache Airflow, data warehousing on Snowflake, and big data processing with PySpark and Hadoop. Whether you need to migrate data, process millions of records daily, or build real-time analytics pipelines, I deliver solutions that ensure data accuracy and reliability.</p>
                <div class="hero-cta">
                    <a href="https://calendly.com/hlobato/lets-talk" class="primary-cta">Schedule a Free Consultation</a>
                    <a href="mailto:iklobato1@gmail.com" class="secondary-cta">Email Me About Your Project</a>
                </div>
            </div>
        </div>
    </section>
    
    <section id="problems-solved">
        <h2>What Problems I Solve</h2>
        
        <div class="services-grid">
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-tasks"></i></div>
                <h3>Manual Data Processing</h3>
                <p>Automate repetitive data processing tasks with reliable pipelines that run on schedule and handle errors gracefully.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-exclamation-triangle"></i></div>
                <h3>Data Quality Issues</h3>
                <p>Implement comprehensive data validation and quality checks that ensure 99.9%+ accuracy in your data pipelines.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-chart-line"></i></div>
                <h3>Scaling Data Processing</h3>
                <p>Design pipelines that scale to handle growing data volumes from thousands to millions of records without performance degradation.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-exchange-alt"></i></div>
                <h3>Complex Data Transformations</h3>
                <p>Handle complex schema transformations, data type conversions, and business logic across multiple heterogeneous data sources.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-clock"></i></div>
                <h3>Real-Time vs Batch Processing</h3>
                <p>Design the right processing strategy for your use case, whether it's real-time streaming or efficient batch processing.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-database"></i></div>
                <h3>Data Warehouse Optimization</h3>
                <p>Optimize data warehouse performance and costs through efficient data modeling, partitioning, and query optimization.</p>
            </div>
        </div>
    </section>
    
    <section id="value-delivered">
        <h2>Value I Deliver</h2>
        
        <div class="value-props">
            <div class="value-prop">
                <i class="fas fa-check-double"></i>
                <h4>99.9% Data Accuracy</h4>
                <p>Comprehensive validation frameworks ensure data quality and accuracy throughout the pipeline, catching errors before they impact downstream systems.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-tachometer-alt"></i>
                <h4>45%+ Performance Improvement</h4>
                <p>Optimize pipeline performance through parallel processing, efficient data transformations, and optimized data warehouse queries.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-robot"></i>
                <h4>Automated Processing</h4>
                <p>Eliminate manual data processing work with automated pipelines that run on schedule, handle failures, and send alerts when issues occur.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-bolt"></i>
                <h4>Real-Time Data Availability</h4>
                <p>Enable faster decision-making with real-time or near-real-time data pipelines that make data available as soon as it's processed.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-expand-arrows-alt"></i>
                <h4>Scalable Architecture</h4>
                <p>Design pipelines that scale from thousands to millions of records without requiring major architectural changes.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-dollar-sign"></i>
                <h4>Cost-Effective Solutions</h4>
                <p>Optimize data warehouse costs through efficient data modeling, partitioning strategies, and right-sized compute resources.</p>
            </div>
        </div>
    </section>
    
    <section id="case-studies">
        <h2>Real-World Implementations</h2>
        
        <div class="case-study">
            <div class="case-study-header">
                <h3>Enterprise Data Migration ETL Pipeline</h3>
                <p class="case-study-meta">OneTrust | Apache Airflow Orchestration | 99.9% Accuracy</p>
            </div>
            
            <div class="case-study-content">
                <div class="case-challenge">
                    <h4>Challenge</h4>
                    <p>Following OneTrust's acquisition of Convercent, we needed to migrate terabytes of sensitive compliance data across different database schemas. The challenge involved handling 50+ complex data types, maintaining 99.9% data accuracy, ensuring zero privacy violations, and completing the migration within tight deadlines for 20+ enterprise clients.</p>
                </div>
                
                <div class="case-solution">
                    <h4>Solution</h4>
                    <p>I designed and implemented an enterprise-grade ETL pipeline orchestrated with Apache Airflow:</p>
                    
                    <ul class="solution-details">
                        <li>Apache Airflow DAGs for workflow orchestration and scheduling</li>
                        <li>Flexible mapping engine handling schema transformations across 50+ data types</li>
                        <li>Parallel batch processing architecture for performance optimization</li>
                        <li>Multi-layer data validation at extraction, transformation, and loading stages</li>
                        <li>Comprehensive error handling and retry mechanisms</li>
                        <li>Data lineage tracking for audit and compliance requirements</li>
                        <li>Monitoring and alerting for pipeline health and data quality issues</li>
                    </ul>
                </div>
                
                <div class="case-results">
                    <h4>Results</h4>
                    <div class="results-metrics">
                        <div class="metric">
                            <div class="metric-value">99.9%</div>
                            <div class="metric-label">Data Accuracy</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">45%</div>
                            <div class="metric-label">Faster Processing</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">20+</div>
                            <div class="metric-label">Clients Migrated</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">0</div>
                            <div class="metric-label">Privacy Violations</div>
                        </div>
                    </div>
                    <p>The ETL pipeline successfully migrated 20+ enterprise clients with zero data loss incidents and zero privacy violations. Parallel batch processing reduced data processing time by 45% while maintaining strict data integrity requirements. The automated orchestration enabled reliable, repeatable migrations that accelerated time-to-value for the acquired customer base.</p>
                </div>
                
                <div class="case-technologies">
                    <h4>Technologies Used</h4>
                    <div class="tech-list">
                        <span class="tech-item">Apache Airflow</span>
                        <span class="tech-item">Python</span>
                        <span class="tech-item">PostgreSQL</span>
                        <span class="tech-item">Django</span>
                        <span class="tech-item">AWS</span>
                        <span class="tech-item">Docker</span>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="case-study">
            <div class="case-study-header">
                <h3>Resilient Data Ingestion with Snowflake</h3>
                <p class="case-study-meta">SecurityScorecard | Airflow + Snowflake | Heterogeneous Data Sources</p>
            </div>
            
            <div class="case-study-content">
                <div class="case-challenge">
                    <h4>Challenge</h4>
                    <p>SecurityScorecard needed to ingest data from multiple heterogeneous sources (APIs, databases, files) into a centralized data warehouse. The challenge was to create resilient pipelines that could handle source failures, schema changes, and varying data volumes while ensuring data quality and timely availability for analytics.</p>
                </div>
                
                <div class="case-solution">
                    <h4>Solution</h4>
                    <p>I created resilient ingestion pipelines orchestrated with Apache Airflow, integrating with Snowflake for data warehousing:</p>
                    
                    <ul class="solution-details">
                        <li>Apache Airflow DAGs for orchestrating multi-source data ingestion</li>
                        <li>Snowflake data warehouse for scalable, performant data storage</li>
                        <li>Resilient error handling with automatic retries and dead-letter queues</li>
                        <li>Schema evolution handling for changing source data structures</li>
                        <li>Incremental loading strategies to minimize processing time</li>
                        <li>Data quality checks and validation before loading to Snowflake</li>
                        <li>Monitoring dashboards for pipeline health and data freshness</li>
                    </ul>
                </div>
                
                <div class="case-results">
                    <h4>Results</h4>
                    <div class="results-metrics">
                        <div class="metric">
                            <div class="metric-value">100%</div>
                            <div class="metric-label">Source Coverage</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">Resilient</div>
                            <div class="metric-label">Error Handling</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">Real-Time</div>
                            <div class="metric-label">Data Availability</div>
                        </div>
                    </div>
                    <p>The pipelines successfully integrated data from all heterogeneous sources into Snowflake, providing a unified view for analytics. The resilient architecture handles source failures gracefully, and the Snowflake integration enables fast, scalable analytics queries. The system supports the platform's ability to handle 3x more vendors per customer.</p>
                </div>
                
                <div class="case-technologies">
                    <h4>Technologies Used</h4>
                    <div class="tech-list">
                        <span class="tech-item">Apache Airflow</span>
                        <span class="tech-item">Snowflake</span>
                        <span class="tech-item">Python</span>
                        <span class="tech-item">AWS</span>
                        <span class="tech-item">Docker</span>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="case-study">
            <div class="case-study-header">
                <h3>Big Data ML Pipeline Processing</h3>
                <p class="case-study-meta">Dextra Digital | PySpark/Hadoop | 10M+ Daily Records</p>
            </div>
            
            <div class="case-study-content">
                <div class="case-challenge">
                    <h4>Challenge</h4>
                    <p>Serasa Experian, through Dextra Digital, needed to process 10M+ daily records for machine learning pipelines. The challenge was to build scalable data processing pipelines that could handle large volumes efficiently, support ML model training, and provide reliable data for analytics and decision-making.</p>
                </div>
                
                <div class="case-solution">
                    <h4>Solution</h4>
                    <p>I built PySpark/Hadoop ML pipelines for big data processing:</p>
                    
                    <ul class="solution-details">
                        <li>PySpark for distributed data processing across large datasets</li>
                        <li>Hadoop ecosystem for scalable storage and processing</li>
                        <li>ML pipeline orchestration for feature engineering and model training</li>
                        <li>Optimized Spark jobs for efficient resource utilization</li>
                        <li>Data partitioning strategies for parallel processing</li>
                        <li>Automated pipeline scheduling and monitoring</li>
                        <li>Integration with ML frameworks for model training and inference</li>
                    </ul>
                </div>
                
                <div class="case-results">
                    <h4>Results</h4>
                    <div class="results-metrics">
                        <div class="metric">
                            <div class="metric-value">10M+</div>
                            <div class="metric-label">Daily Records</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">Scalable</div>
                            <div class="metric-label">Architecture</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">ML-Ready</div>
                            <div class="metric-label">Data Pipeline</div>
                        </div>
                    </div>
                    <p>The PySpark/Hadoop pipelines successfully process 10M+ daily records, providing clean, processed data for ML model training and analytics. The scalable architecture handles growing data volumes, and the ML pipeline integration enables automated feature engineering and model training workflows.</p>
                </div>
                
                <div class="case-technologies">
                    <h4>Technologies Used</h4>
                    <div class="tech-list">
                        <span class="tech-item">PySpark</span>
                        <span class="tech-item">Hadoop</span>
                        <span class="tech-item">Python</span>
                        <span class="tech-item">ML Pipelines</span>
                        <span class="tech-item">Big Data</span>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <section id="technologies">
        <h2>Technologies & Tools I Work With</h2>
        
        <div class="tech-container">
            <div class="tech-category">
                <h3>Orchestration</h3>
                <div class="tech-list">
                    <span class="tech-item">Apache Airflow</span>
                    <span class="tech-item">Prefect</span>
                    <span class="tech-item">Dagster</span>
                    <span class="tech-item">Luigi</span>
                </div>
            </div>
            
            <div class="tech-category">
                <h3>Data Warehouses</h3>
                <div class="tech-list">
                    <span class="tech-item">Snowflake</span>
                    <span class="tech-item">BigQuery</span>
                    <span class="tech-item">Redshift</span>
                    <span class="tech-item">Data Lakes</span>
                </div>
            </div>
            
            <div class="tech-category">
                <h3>Big Data Processing</h3>
                <div class="tech-list">
                    <span class="tech-item">Apache Spark</span>
                    <span class="tech-item">PySpark</span>
                    <span class="tech-item">Hadoop</span>
                    <span class="tech-item">Hive</span>
                </div>
            </div>
            
            <div class="tech-category">
                <h3>Databases</h3>
                <div class="tech-list">
                    <span class="tech-item">PostgreSQL</span>
                    <span class="tech-item">MySQL</span>
                    <span class="tech-item">MongoDB</span>
                    <span class="tech-item">DynamoDB</span>
                </div>
            </div>
            
            <div class="tech-category">
                <h3>Cloud Storage</h3>
                <div class="tech-list">
                    <span class="tech-item">AWS S3</span>
                    <span class="tech-item">GCS</span>
                    <span class="tech-item">Azure Blob</span>
                    <span class="tech-item">Data Lakes</span>
                </div>
            </div>
            
            <div class="tech-category">
                <h3>Data Quality</h3>
                <div class="tech-list">
                    <span class="tech-item">Great Expectations</span>
                    <span class="tech-item">dbt</span>
                    <span class="tech-item">Custom Validators</span>
                    <span class="tech-item">Data Profiling</span>
                </div>
            </div>
        </div>
    </section>
    
    <section id="how-i-work">
        <h2>How I Work</h2>
        
        <div class="process-steps">
            <div class="process-step">
                <div class="step-number">1</div>
                <h3>Data Requirements Analysis</h3>
                <p>I analyze your data sources, understand data volumes, identify transformation requirements, and define data quality standards. This includes mapping source to target schemas and identifying business rules.</p>
            </div>
            
            <div class="process-step">
                <div class="step-number">2</div>
                <h3>Pipeline Architecture Design</h3>
                <p>I design the pipeline architecture choosing between ETL and ELT patterns based on your needs. This includes selecting appropriate tools, designing data flow, and planning for scalability and reliability.</p>
            </div>
            
            <div class="process-step">
                <div class="step-number">3</div>
                <h3>Data Quality Framework</h3>
                <p>I implement comprehensive data validation and quality checks at multiple stages of the pipeline. This includes schema validation, data type checks, business rule validation, and anomaly detection.</p>
            </div>
            
            <div class="process-step">
                <div class="step-number">4</div>
                <h3>Orchestration & Scheduling</h3>
                <p>I set up workflow orchestration using Apache Airflow or similar tools, configure scheduling, implement error handling and retries, and set up dependencies between pipeline tasks.</p>
            </div>
            
            <div class="process-step">
                <div class="step-number">5</div>
                <h3>Monitoring & Optimization</h3>
                <p>I implement monitoring and alerting for pipeline health, data quality issues, and performance metrics. I also optimize pipeline performance through tuning and optimization.</p>
            </div>
        </div>
    </section>
    
    <section id="engagement-models">
        <h2>How We Can Work Together</h2>
        
        <div class="services-grid">
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-project-diagram"></i></div>
                <h3>End-to-End Pipeline Development</h3>
                <p>Complete pipeline development from design to deployment, including data quality frameworks and monitoring setup.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-tools"></i></div>
                <h3>Pipeline Optimization</h3>
                <p>Optimize existing pipelines for performance, cost, and reliability. Refactor legacy pipelines to modern architectures.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-search"></i></div>
                <h3>Data Quality Audits</h3>
                <p>Assess data quality issues, implement validation frameworks, and improve data accuracy in existing pipelines.</p>
            </div>
            
            <div class="service-card">
                <div class="service-icon"><i class="fas fa-headset"></i></div>
                <h3>Ongoing Pipeline Maintenance</h3>
                <p>Monthly retainer for ongoing pipeline maintenance, optimization, and support for your data engineering needs.</p>
            </div>
        </div>
    </section>
    
    <section id="why-choose-me">
        <h2>Why Choose Me</h2>
        
        <div class="value-props">
            <div class="value-prop">
                <i class="fas fa-database"></i>
                <h4>Petabyte-Scale Experience</h4>
                <p>Experience building and optimizing data pipelines that process petabytes of data across various industries and use cases.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-check-double"></i>
                <h4>Proven Accuracy</h4>
                <p>Consistent track record of achieving 99.9%+ data accuracy through comprehensive validation and quality frameworks.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-tachometer-alt"></i>
                <h4>Performance Optimization</h4>
                <p>Expertise in optimizing pipeline performance, achieving 45%+ improvements in processing time through efficient design.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-building"></i>
                <h4>Enterprise Experience</h4>
                <p>Proven experience with enterprise data pipeline requirements including compliance, security, and scalability needs.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-shield-alt"></i>
                <h4>Reliability Focus</h4>
                <p>Design pipelines with reliability as a core principle, including error handling, monitoring, and disaster recovery.</p>
            </div>
            <div class="value-prop">
                <i class="fas fa-code"></i>
                <h4>Maintainable Solutions</h4>
                <p>Build pipelines that are easy to understand, maintain, and extend, reducing long-term operational costs.</p>
            </div>
        </div>
    </section>
    
    <section id="faq">
        <h2>Frequently Asked Questions</h2>
        
        <div class="faq-container">
            <div class="faq-item">
                <h3>How do you ensure data quality in pipelines?</h3>
                <div class="faq-answer">
                    <p>I implement multi-layer data validation including schema validation, data type checks, business rule validation, and anomaly detection. Validation occurs at extraction, transformation, and loading stages. I use frameworks like Great Expectations and custom validators to catch data quality issues before they impact downstream systems. This approach has consistently achieved 99.9%+ data accuracy.</p>
                </div>
            </div>
            
            <div class="faq-item">
                <h3>What's the difference between ETL and ELT?</h3>
                <div class="faq-answer">
                    <p>ETL (Extract, Transform, Load) transforms data before loading into the data warehouse, while ELT (Extract, Load, Transform) loads raw data first and transforms it within the warehouse. I help choose the right approach based on your data warehouse capabilities, transformation complexity, and performance requirements. Modern cloud data warehouses like Snowflake excel at ELT patterns.</p>
                </div>
            </div>
            
            <div class="faq-item">
                <h3>How do you handle data pipeline failures?</h3>
                <div class="faq-answer">
                    <p>I implement comprehensive error handling including automatic retries with exponential backoff, dead-letter queues for failed records, checkpointing to resume from failures, and alerting for critical issues. Pipelines are designed to be idempotent, allowing safe retries without data duplication. I also implement data lineage tracking to identify and fix issues quickly.</p>
                </div>
            </div>
            
            <div class="faq-item">
                <h3>Can you work with our existing data warehouse?</h3>
                <div class="faq-answer">
                    <p>Yes, I have experience with Snowflake, BigQuery, Redshift, and traditional data warehouses. I can build pipelines that integrate with your existing data warehouse infrastructure, optimize performance, and recommend improvements where appropriate. I can also help migrate between data warehouses if needed.</p>
                </div>
            </div>
            
            <div class="faq-item">
                <h3>What monitoring do you set up for pipelines?</h3>
                <div class="faq-answer">
                    <p>I implement comprehensive monitoring including pipeline execution status, data quality metrics, processing times, error rates, and data freshness. This includes dashboards for visibility, alerts for failures and data quality issues, and automated notifications. Monitoring is set up using tools like Airflow's built-in monitoring, CloudWatch, or custom dashboards depending on your infrastructure.</p>
                </div>
            </div>
            
            <div class="faq-item">
                <h3>How do you handle schema changes in source data?</h3>
                <div class="faq-answer">
                    <p>I design pipelines with schema evolution in mind, using flexible mapping engines that can handle schema changes gracefully. This includes schema versioning, backward compatibility checks, and automated schema detection. For critical changes, I implement validation and alerting to ensure schema changes don't break downstream processes.</p>
                </div>
            </div>
        </div>
    </section>
    
    <section id="contact">
        <div class="contact-container">
            <h2>Ready to Build Reliable Data Pipelines?</h2>
            <p>Let's discuss how I can help you build data pipelines that ensure data accuracy and reliability at scale.</p>
            
            <div class="contact-cta">
                <a href="https://calendly.com/hlobato/lets-talk" class="primary-cta">Schedule a Free Consultation</a>
                <a href="mailto:iklobato1@gmail.com" class="secondary-cta">Email Me About Your Project</a>
            </div>
        </div>
    </section>
</main>

<style>
    .breadcrumb {
        margin: 1rem 0;
        font-size: 0.9rem;
    }
    
    .breadcrumb ol {
        display: flex;
        list-style: none;
        padding: 0;
        margin: 0;
    }
    
    .breadcrumb li:not(:last-child)::after {
        content: '/';
        margin: 0 0.5rem;
        color: var(--light-text);
    }
    
    .breadcrumb a {
        color: var(--primary-color);
        text-decoration: none;
    }
    
    .breadcrumb a:hover {
        text-decoration: underline;
    }
    
    .service-hero {
        margin: 2rem 0 3rem;
    }
    
    .service-hero-content h2 {
        margin-top: 0;
        margin-bottom: 1.5rem;
        font-size: 1.8rem;
        border-bottom: none;
    }
    
    .service-hero-content p {
        font-size: 1.1rem;
        line-height: 1.6;
        margin-bottom: 1.5rem;
    }
    
    .hero-cta {
        display: flex;
        flex-direction: column;
        gap: 1rem;
        margin-top: 1rem;
    }
    
    .primary-cta, .secondary-cta {
        display: inline-block;
        padding: 0.8rem 1.5rem;
        border-radius: 4px;
        text-decoration: none;
        font-weight: 600;
        text-align: center;
        transition: all 0.3s ease;
    }
    
    .primary-cta {
        background-color: var(--primary-color);
        color: var(--white);
    }
    
    .primary-cta:hover {
        background-color: #0550ae;
    }
    
    .secondary-cta {
        background-color: var(--card-bg);
        color: var(--primary-color);
        border: 2px solid var(--primary-color);
    }
    
    .secondary-cta:hover {
        background-color: var(--highlight-bg);
    }
    
    .case-study {
        background-color: var(--card-bg);
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 4px 16px rgba(0,0,0,0.15);
        border: 1px solid var(--border-color);
        margin-top: 2rem;
    }
    
    .case-study-header {
        background-color: var(--highlight-bg);
        padding: 1.5rem 2rem;
        border-bottom: 1px solid var(--border-color);
    }
    
    .case-study-header h3 {
        margin: 0;
        font-size: 1.4rem;
    }
    
    .case-study-meta {
        color: var(--light-text);
        margin: 0.5rem 0 0;
        font-size: 0.9rem;
    }
    
    .case-study-content {
        padding: 2rem;
    }
    
    .case-challenge, .case-solution, .case-results, .case-technologies {
        margin-bottom: 2rem;
    }
    
    .case-study-content h4 {
        color: var(--primary-color);
        margin-top: 0;
        margin-bottom: 1rem;
        font-size: 1.2rem;
    }
    
    .solution-details {
        margin: 1rem 0;
        padding-left: 1.5rem;
    }
    
    .solution-details li {
        margin-bottom: 0.5rem;
    }
    
    .results-metrics {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin: 1.5rem 0;
    }
    
    .metric {
        text-align: center;
        background-color: var(--highlight-bg);
        padding: 1rem;
        border-radius: 8px;
        min-width: 100px;
        flex: 1;
    }
    
    .metric-value {
        font-size: 1.8rem;
        font-weight: 700;
        color: var(--primary-color);
        margin-bottom: 0.5rem;
    }
    
    .metric-label {
        font-size: 0.9rem;
        color: var(--light-text);
    }
    
    .case-technologies {
        padding-top: 1rem;
        border-top: 1px solid var(--border-color);
    }
    
    .tech-container {
        display: grid;
        grid-template-columns: 1fr;
        gap: 2rem;
        margin-top: 2rem;
    }
    
    .tech-category h3 {
        margin-top: 0;
        margin-bottom: 1rem;
        font-size: 1.2rem;
    }
    
    .tech-list {
        display: flex;
        flex-wrap: wrap;
        gap: 0.75rem;
    }
    
    .tech-item {
        display: inline-block;
        background-color: var(--highlight-bg);
        padding: 0.3rem 0.8rem;
        border-radius: 4px;
        font-size: 0.9rem;
        color: var(--text-color);
        font-weight: 500;
        border: 1px solid var(--border-color);
    }
    
    .process-steps {
        display: grid;
        grid-template-columns: 1fr;
        gap: 2rem;
        margin-top: 2rem;
    }
    
    .process-step {
        display: flex;
        gap: 1.5rem;
        align-items: flex-start;
    }
    
    .step-number {
        flex-shrink: 0;
        width: 50px;
        height: 50px;
        background-color: var(--primary-color);
        color: var(--white);
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.5rem;
        font-weight: 700;
    }
    
    .process-step h3 {
        margin-top: 0;
        margin-bottom: 0.5rem;
    }
    
    .faq-container {
        margin-top: 2rem;
    }
    
    .faq-item {
        border-bottom: 1px solid var(--border-color);
        padding: 1.5rem 0;
    }
    
    .faq-item:last-child {
        border-bottom: none;
    }
    
    .faq-item h3 {
        margin: 0;
        font-size: 1.1rem;
    }
    
    .faq-answer {
        margin-top: 1rem;
    }
    
    .contact-container {
        background-color: var(--highlight-bg);
        padding: 2.5rem;
        border-radius: 8px;
        text-align: center;
        margin: 3rem 0;
        border: 1px solid var(--border-color);
    }
    
    .contact-container h2 {
        margin-top: 0;
        border-bottom: none;
    }
    
    .contact-container p {
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    .contact-cta {
        display: flex;
        flex-direction: column;
        gap: 1rem;
        max-width: 500px;
        margin: 0 auto;
    }
    
    @media (min-width: 768px) {
        .services-grid {
            grid-template-columns: repeat(2, 1fr);
        }
        
        .hero-cta, .contact-cta {
            flex-direction: row;
            justify-content: center;
        }
        
        .tech-container {
            grid-template-columns: repeat(2, 1fr);
        }
        
        .process-steps {
            grid-template-columns: 1fr;
        }
    }
    
    @media (min-width: 992px) {
        .services-grid {
            grid-template-columns: repeat(3, 1fr);
        }
        
        .tech-container {
            grid-template-columns: repeat(3, 1fr);
        }
    }
</style>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Service",
  "serviceType": "Data Pipeline Engineering",
  "provider": {
    "@type": "Person",
    "name": "Henrique Lobato",
    "url": "https://iklobato.github.io"
  },
  "description": "Data pipeline engineering services using Apache Airflow, Snowflake, PySpark, and Hadoop. Build reliable ETL/ELT pipelines that process millions of records with 99.9% accuracy.",
  "areaServed": {
    "@type": "Country",
    "name": "Worldwide"
  }
}
</script>

